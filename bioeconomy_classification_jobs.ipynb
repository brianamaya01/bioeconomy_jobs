{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import spacy.util\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Limpieza de las csv jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_andres = pd.read_csv(\"C:/Users/brian.amaya/Documents/2.Proyectos/jobs_bioeconomy/raw_data/TestGreenScore.csv\")\n",
    "jobs_andres = jobs_andres [[\"ID_file\", \"title_kw\",\"title_kw_des\"]]\n",
    "\n",
    "jobs_linkedin = pd.read_csv(\"C:/Users/brian.amaya/Documents/2.Proyectos/Bedex/dash_news/raw_data/jobs/job_details.csv\")\n",
    "jobs_linkedin = jobs_linkedin [[\"keyword\",\"current_jobid\", \"job_title\",\"description_job\"]]\n",
    "jobs_linkedin = jobs_linkedin.rename(columns={\n",
    "    \"current_jobid\": \"ID_file\",\n",
    "    \"job_title\": \"title_kw\",\n",
    "    \"description_job\": \"title_kw_des\"\n",
    "})\n",
    "\n",
    "# Ejemplo de uso\n",
    "\n",
    "#jobs_df = jobs_andres\n",
    "#jobs_df = jobs_linkedin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([char for char in nfkd_form if not unicodedata.combining(char)])  # Normalizar el texto eliminando tildes\n",
    "\n",
    "\n",
    "jobs_linkedin['title_kw_des'] = jobs_linkedin['title_kw_des'].apply(remove_accents)\n",
    "\n",
    "# Cargar los modelos de spaCy en español y en inglés\n",
    "nlp_es = spacy.load(\"es_core_news_md\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Cargar los archivos CSV con las palabras clave y sectores en español e inglés\n",
    "df_keywords_es = pd.read_csv('tidy_data/in_es_keywords.csv')\n",
    "df_keywords_en = pd.read_csv('tidy_data/in_en_keywords.csv')\n",
    "\n",
    "# Limpiar y preparar los datos del DataFrame de palabras clave en español\n",
    "df_keywords_es['sectors'] = df_keywords_es['sectors'].fillna('Unknown')\n",
    "df_keywords_es['sectors'] = df_keywords_es['sectors'].astype(str)\n",
    "\n",
    "# Organizar las palabras clave por sector en español\n",
    "keywords_by_sector_es = {}\n",
    "for index, row in df_keywords_es.iterrows():\n",
    "    sector = row['sectors']\n",
    "    keyword = remove_accents(row['keyword_es'])  # Normalizar palabras clave eliminando tildes\n",
    "    if sector not in keywords_by_sector_es:\n",
    "        keywords_by_sector_es[sector] = []\n",
    "    keywords_by_sector_es[sector].append(keyword)\n",
    "\n",
    "# Limpiar y preparar los datos del DataFrame de palabras clave en inglés\n",
    "df_keywords_en['sectors'] = df_keywords_en['sectors'].fillna('Unknown')\n",
    "df_keywords_en['sectors'] = df_keywords_en['sectors'].astype(str)\n",
    "\n",
    "# Organizar las palabras clave por sector en inglés\n",
    "keywords_by_sector_en = {}\n",
    "for index, row in df_keywords_en.iterrows():\n",
    "    sector = row['sectors']\n",
    "    keyword = remove_accents(row['keyword_en'])  # Normalizar palabras clave eliminando tildes\n",
    "    if sector not in keywords_by_sector_en:\n",
    "        keywords_by_sector_en[sector] = []\n",
    "    keywords_by_sector_en[sector].append(keyword)\n",
    "\n",
    "def process_jobs(jobs_df, language='es'):\n",
    "    # Identificar el idioma de cada descripción\n",
    "    jobs_df['language'] = jobs_df['title_kw_des'].apply(lambda x: detect(x))\n",
    "\n",
    "    # Filtrar los DataFrames según el idioma\n",
    "    df_es = jobs_df[jobs_df['language'] == 'es']\n",
    "    df_en = jobs_df[jobs_df['language'] == 'en']\n",
    "\n",
    "    if language == 'es':\n",
    "        matcher = PhraseMatcher(nlp_es.vocab, attr=\"LEMMA\")\n",
    "        for sector, keywords in keywords_by_sector_es.items():\n",
    "            patterns = [nlp_es(text.lower()) for text in keywords]\n",
    "            matcher.add(sector.upper(), None, *patterns)\n",
    "        \n",
    "        nlp = nlp_es\n",
    "        df_to_process = df_es\n",
    "        keywords_by_sector = keywords_by_sector_es\n",
    "    else:\n",
    "        matcher = PhraseMatcher(nlp_en.vocab, attr=\"LEMMA\")\n",
    "        for sector, keywords in keywords_by_sector_en.items():\n",
    "            patterns = [nlp_en(text.lower()) for text in keywords]\n",
    "            matcher.add(sector.upper(), None, *patterns)\n",
    "        \n",
    "        nlp = nlp_en\n",
    "        df_to_process = df_en\n",
    "        keywords_by_sector = keywords_by_sector_en\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for index, row in df_to_process.iterrows():\n",
    "        description = row['title_kw_des']\n",
    "        \n",
    "        # Procesar el texto\n",
    "        doc = nlp(description.lower())\n",
    "\n",
    "        # Encontrar las coincidencias y etiquetarlas\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            if not any(span.start <= start < span.end or span.start < end <= span.end for span in spans):\n",
    "                spans.append(Span(doc, start, end, label=nlp.vocab.strings[match_id]))\n",
    "\n",
    "        # Eliminar duplicados y solapamientos potenciales\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        doc.ents = spans\n",
    "\n",
    "        # Extraer datos para el DataFrame\n",
    "        data = {sector.upper(): [] for sector in keywords_by_sector}\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ not in data:\n",
    "                data[ent.label_] = []\n",
    "            data[ent.label_].append(ent.text)\n",
    "\n",
    "        # Convertir listas de palabras en cadenas separadas por comas\n",
    "        for sector in data:\n",
    "            data[sector] = ', '.join(set(data[sector]))\n",
    "\n",
    "        # Agregar los datos del DataFrame original\n",
    "        data['ID_file'] = row['ID_file']\n",
    "        data['title_kw'] = row['title_kw']\n",
    "        data['Processed_Text'] = description\n",
    "        processed_data.append(data)\n",
    "\n",
    "    # Crear DataFrame con la información recopilada\n",
    "    df_final = pd.DataFrame(processed_data)\n",
    "    # Reordenar columnas\n",
    "    df_final = df_final[['ID_file', 'title_kw', 'Processed_Text'] + [col for col in df_final.columns if col not in ['ID_file', 'title_kw', 'Processed_Text']]]\n",
    "\n",
    "    # Crear columnas de conteo\n",
    "    for sector in keywords_by_sector:\n",
    "        df_final[sector.upper() + '_COUNT'] = df_final[sector.upper()].apply(lambda x: len(x.split(', ')) if x else 0)\n",
    "\n",
    "    # Guardar el DataFrame en un archivo CSV\n",
    "    df_final.to_csv(f'processed_jobs_{language}.csv', index=False)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Procesar y guardar DataFrames para español e inglés\n",
    "df_es = process_jobs(jobs_linkedin, language='es')\n",
    "df_en = process_jobs(jobs_linkedin, language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import unicodedata\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from langdetect import detect\n",
    "\n",
    "# Función para eliminar tildes\n",
    "def remove_accents(text):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([char for char in nfkd_form if not unicodedata.combining(char)])\n",
    "\n",
    "jobs_linkedin['title_kw_des'] = jobs_linkedin['title_kw_des'].apply(remove_accents)\n",
    "\n",
    "# Cargar los modelos de spaCy en español, inglés y portugués\n",
    "nlp_es = spacy.load(\"es_core_news_md\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_pt = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Cargar los archivos CSV con las palabras clave y sectores en español, inglés y portugués\n",
    "df_keywords_es = pd.read_csv('tidy_data/in_es_keywords.csv')\n",
    "df_keywords_en = pd.read_csv('tidy_data/in_en_keywords.csv')\n",
    "df_keywords_pt = pd.read_csv('tidy_data/in_pt_keywords.csv')\n",
    "\n",
    "# Limpiar y preparar los datos del DataFrame de palabras clave en español\n",
    "df_keywords_es['sectors'] = df_keywords_es['sectors'].fillna('Unknown')\n",
    "df_keywords_es['sectors'] = df_keywords_es['sectors'].astype(str)\n",
    "\n",
    "# Organizar las palabras clave por sector en español\n",
    "keywords_by_sector_es = {}\n",
    "for index, row in df_keywords_es.iterrows():\n",
    "    sector = row['sectors']\n",
    "    keyword = remove_accents(row['keyword_es'])  # Normalizar palabras clave eliminando tildes\n",
    "    if sector not in keywords_by_sector_es:\n",
    "        keywords_by_sector_es[sector] = []\n",
    "    keywords_by_sector_es[sector].append(keyword)\n",
    "\n",
    "# Limpiar y preparar los datos del DataFrame de palabras clave en inglés\n",
    "df_keywords_en['sectors'] = df_keywords_en['sectors'].fillna('Unknown')\n",
    "df_keywords_en['sectors'] = df_keywords_en['sectors'].astype(str)\n",
    "\n",
    "# Organizar las palabras clave por sector en inglés\n",
    "keywords_by_sector_en = {}\n",
    "for index, row in df_keywords_en.iterrows():\n",
    "    sector = row['sectors']\n",
    "    keyword = remove_accents(row['keyword_en'])  # Normalizar palabras clave eliminando tildes\n",
    "    if sector not in keywords_by_sector_en:\n",
    "        keywords_by_sector_en[sector] = []\n",
    "    keywords_by_sector_en[sector].append(keyword)\n",
    "\n",
    "# Limpiar y preparar los datos del DataFrame de palabras clave en portugués\n",
    "df_keywords_pt['sectors'] = df_keywords_pt['sectors'].fillna('Unknown')\n",
    "df_keywords_pt['sectors'] = df_keywords_pt['sectors'].astype(str)\n",
    "\n",
    "# Organizar las palabras clave por sector en portugués\n",
    "keywords_by_sector_pt = {}\n",
    "for index, row in df_keywords_pt.iterrows():\n",
    "    sector = row['sectors']\n",
    "    keyword = remove_accents(row['keyword_pt'])  # Normalizar palabras clave eliminando tildes\n",
    "    if sector not in keywords_by_sector_pt:\n",
    "        keywords_by_sector_pt[sector] = []\n",
    "    keywords_by_sector_pt[sector].append(keyword)\n",
    "\n",
    "def process_jobs(jobs_df, language='es'):\n",
    "    # Identificar el idioma de cada descripción\n",
    "    jobs_df['language'] = jobs_df['title_kw_des'].apply(lambda x: detect(x))\n",
    "\n",
    "    # Filtrar los DataFrames según el idioma\n",
    "    df_es = jobs_df[jobs_df['language'] == 'es']\n",
    "    df_en = jobs_df[jobs_df['language'] == 'en']\n",
    "    df_pt = jobs_df[jobs_df['language'] == 'pt']\n",
    "\n",
    "    if language == 'es':\n",
    "        matcher = PhraseMatcher(nlp_es.vocab, attr=\"LEMMA\")\n",
    "        for sector, keywords in keywords_by_sector_es.items():\n",
    "            patterns = [nlp_es(text.lower()) for text in keywords]\n",
    "            matcher.add(sector.upper(), None, *patterns)\n",
    "        \n",
    "        nlp = nlp_es\n",
    "        df_to_process = df_es\n",
    "        keywords_by_sector = keywords_by_sector_es\n",
    "    elif language == 'en':\n",
    "        matcher = PhraseMatcher(nlp_en.vocab, attr=\"LEMMA\")\n",
    "        for sector, keywords in keywords_by_sector_en.items():\n",
    "            patterns = [nlp_en(text.lower()) for text in keywords]\n",
    "            matcher.add(sector.upper(), None, *patterns)\n",
    "        \n",
    "        nlp = nlp_en\n",
    "        df_to_process = df_en\n",
    "        keywords_by_sector = keywords_by_sector_en\n",
    "    elif language == 'pt':\n",
    "        matcher = PhraseMatcher(nlp_pt.vocab, attr=\"LEMMA\")\n",
    "        for sector, keywords in keywords_by_sector_pt.items():\n",
    "            patterns = [nlp_pt(text.lower()) for text in keywords]\n",
    "            matcher.add(sector.upper(), None, *patterns)\n",
    "        \n",
    "        nlp = nlp_pt\n",
    "        df_to_process = df_pt\n",
    "        keywords_by_sector = keywords_by_sector_pt\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for index, row in df_to_process.iterrows():\n",
    "        description = row['title_kw_des']\n",
    "        \n",
    "        # Procesar el texto\n",
    "        doc = nlp(description.lower())\n",
    "\n",
    "        # Encontrar las coincidencias y etiquetarlas\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            if not any(span.start <= start < span.end or span.start < end <= span.end for span in spans):\n",
    "                spans.append(Span(doc, start, end, label=nlp.vocab.strings[match_id]))\n",
    "\n",
    "        # Eliminar duplicados y solapamientos potenciales\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        doc.ents = spans\n",
    "\n",
    "        # Extraer datos para el DataFrame\n",
    "        data = {sector.upper(): [] for sector in keywords_by_sector}\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ not in data:\n",
    "                data[ent.label_] = []\n",
    "            data[ent.label_].append(ent.text)\n",
    "\n",
    "        # Convertir listas de palabras en cadenas separadas por comas\n",
    "        for sector in data:\n",
    "            data[sector] = ', '.join(set(data[sector]))\n",
    "\n",
    "        # Agregar los datos del DataFrame original\n",
    "        data['ID_file'] = row['ID_file']\n",
    "        data['title_kw'] = row['title_kw']\n",
    "        data['Processed_Text'] = description\n",
    "        processed_data.append(data)\n",
    "\n",
    "    # Crear DataFrame con la información recopilada\n",
    "    df_final = pd.DataFrame(processed_data)\n",
    "    # Reordenar columnas\n",
    "    df_final = df_final[['ID_file', 'title_kw', 'Processed_Text'] + [col for col in df_final.columns if col not in ['ID_file', 'title_kw', 'Processed_Text']]]\n",
    "\n",
    "    # Crear columnas de conteo\n",
    "    for sector in keywords_by_sector:\n",
    "        df_final[sector.upper() + '_COUNT'] = df_final[sector.upper()].apply(lambda x: len(x.split(', ')) if x else 0)\n",
    "\n",
    "    # Guardar el DataFrame en un archivo CSV\n",
    "    df_final.to_csv(f'processed_jobs_{language}.csv', index=False)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Procesar y guardar DataFrames para español, inglés y portugués\n",
    "df_es = process_jobs(jobs_linkedin, language='es')\n",
    "df_en = process_jobs(jobs_linkedin, language='en')\n",
    "df_pt = process_jobs(jobs_linkedin, language='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column):\n",
    "    all_words = ', '.join(df[column].dropna()).split(', ')\n",
    "    all_words = [word.strip() for word in all_words if word.strip()]  # Eliminar espacios adicionales y vacíos\n",
    "    return Counter(all_words)\n",
    "\n",
    "# Categorías a considerar\n",
    "categories = ['TRANSVERSAL', 'AGRO', 'PISCÍCOLA', 'SALUD', 'BIOTECNOLOGÍA', 'RESIDUOS', 'FORESTAL', 'ENERGÍA', 'TURISMO']\n",
    "\n",
    "# Lista para almacenar los datos procesados\n",
    "processed_data = []\n",
    "\n",
    "# Procesar los datos para cada categoría en español\n",
    "for category in categories:\n",
    "    if category in df_es.columns:\n",
    "        word_counts = count_words(df_es, category)\n",
    "        for word, count in word_counts.items():\n",
    "            processed_data.append({'Categoria': category, 'Palabra': word, 'Conteo': count, 'Language': 'es'})\n",
    "\n",
    "# Procesar los datos para cada categoría en inglés\n",
    "for category in categories:\n",
    "    if category in df_en.columns:\n",
    "        word_counts = count_words(df_en, category)\n",
    "        for word, count in word_counts.items():\n",
    "            processed_data.append({'Categoria': category, 'Palabra': word, 'Conteo': count, 'Language': 'en'})\n",
    "\n",
    "# Crear DataFrame con la información recopilada\n",
    "df_count_es_en = pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_es_en.to_csv('df_count_es_en.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para eliminar tildes\n",
    "def remove_accents(text):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([char for char in nfkd_form if not unicodedata.combining(char)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jobs_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Normalizar el texto eliminando tildes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m jobs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_kw_des\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mjobs_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_kw_des\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_accents)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cargar los modelos de spaCy en español y en inglés\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nlp_es \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes_core_news_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jobs_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalizar el texto eliminando tildes\n",
    "jobs_df['title_kw_des'] = jobs_df['title_kw_des'].apply(remove_accents)\n",
    "\n",
    "# Cargar los modelos de spaCy en español y en inglés\n",
    "nlp_es = spacy.load(\"es_core_news_md\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Cargar el archivo CSV con las palabras clave y sectores\n",
    "df_keywords = pd.read_csv('tidy_data/in_es_keywords.csv')\n",
    "\n",
    "# Limpiar y preparar los datos del DataFrame de palabras clave\n",
    "df_keywords['sectors'] = df_keywords['sectors'].fillna('Unknown')\n",
    "df_keywords['sectors'] = df_keywords['sectors'].astype(str)\n",
    "\n",
    "# Organizar las palabras clave por sector\n",
    "keywords_by_sector = {}\n",
    "for index, row in df_keywords.iterrows():\n",
    "    sector = row['sectors']\n",
    "    keyword = remove_accents(row['keyword_es'])  # Normalizar palabras clave eliminando tildes\n",
    "    if sector not in keywords_by_sector:\n",
    "        keywords_by_sector[sector] = []\n",
    "    keywords_by_sector[sector].append(keyword)\n",
    "\n",
    "# Inicializar el PhraseMatcher y añadir patrones\n",
    "matcher = PhraseMatcher(nlp_es.vocab, attr=\"LEMMA\")\n",
    "for sector, keywords in keywords_by_sector.items():\n",
    "    patterns = [nlp_es(text.lower()) for text in keywords]\n",
    "    matcher.add(sector.upper(), None, *patterns)\n",
    "\n",
    "# Lista para almacenar los datos procesados\n",
    "processed_data = []\n",
    "\n",
    "# Iterar sobre cada descripción en el archivo CSV\n",
    "for index, row in jobs_df.iterrows():\n",
    "    description = row['title_kw_des']\n",
    "    \n",
    "    # Procesar el texto\n",
    "    doc = nlp_es(description.lower())\n",
    "\n",
    "    # Encontrar las coincidencias y etiquetarlas\n",
    "    matches = matcher(doc)\n",
    "    spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        if not any(span.start <= start < span.end or span.start < end <= span.end for span in spans):\n",
    "            spans.append(Span(doc, start, end, label=nlp_es.vocab.strings[match_id]))\n",
    "\n",
    "    # Eliminar duplicados y solapamientos potenciales\n",
    "    spans = spacy.util.filter_spans(spans)\n",
    "    doc.ents = spans\n",
    "\n",
    "    # Extraer datos para el DataFrame\n",
    "    data = {sector.upper(): [] for sector in keywords_by_sector}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in data:\n",
    "            data[ent.label_] = []\n",
    "        data[ent.label_].append(ent.text)\n",
    "\n",
    "    # Convertir listas de palabras en cadenas separadas por comas\n",
    "    for sector in data:\n",
    "        data[sector] = ', '.join(set(data[sector]))\n",
    "\n",
    "    # Agregar los datos del DataFrame original\n",
    "    data['ID_file'] = row['ID_file']\n",
    "    data['title_kw'] = row['title_kw']\n",
    "    data['Processed_Text'] = description\n",
    "    processed_data.append(data)\n",
    "\n",
    "# Crear DataFrame con la información recopilada\n",
    "df_final = pd.DataFrame(processed_data)\n",
    "# Reordenar columnas\n",
    "df_final = df_final[['ID_file', 'title_kw', 'Processed_Text'] + [col for col in df_final.columns if col not in ['ID_file', 'title_kw', 'Processed_Text']]]\n",
    "\n",
    "# Crear columnas de conteo\n",
    "for sector in keywords_by_sector:\n",
    "    df_final[sector.upper() + '_COUNT'] = df_final[sector.upper()].apply(lambda x: len(x.split(', ')) if x else 0)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Tabla de Entidades y Conteos:\")\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV\n",
    "df_final.to_csv('jobs_andres.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a CSV\n",
    "df_final.to_csv('raw_data/jobs_linkedin.csv', index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
